
@article{lee_predicting_2008,
	title = {Predicting {Corporate} 8-{K} {Content} {Using} {Machine} {Learning} {Techniques}},
	journal = {Graduate School of Business Stanford University},
	author = {Lee, JM and Lee, Hyungjun},
	year = {2008},
	file = {Lee and Lee - Predicting Corporate 8-K Content Using Machine Lea.pdf:/Users/jkittler/Zotero/storage/APIPJHYQ/Lee and Lee - Predicting Corporate 8-K Content Using Machine Lea.pdf:application/pdf}
}

@inproceedings{lee_importance_2014,
	title = {On the {Importance} of {Text} {Analysis} for {Stock} {Price} {Prediction}.},
	booktitle = {{LREC}},
	author = {Lee, Heeyoung and Surdeanu, Mihai and MacCartney, Bill and Jurafsky, Dan},
	year = {2014},
	pages = {1170--1175},
	file = {Lee et al. - On the Importance of Text Analysis for Stock Price.pdf:/Users/jkittler/Zotero/storage/DC2ILP4Z/Lee et al. - On the Importance of Text Analysis for Stock Price.pdf:application/pdf}
}

@article{heston_news_2013,
	title = {News versus {Sentiment}: {Comparing} {Textual} {Processing} {Approaches} for {Predicting} {Stock} {Returns}},
	issn = {1556-5068},
	shorttitle = {News versus {Sentiment}},
	url = {http://www.ssrn.com/abstract=2311310},
	doi = {10.2139/ssrn.2311310},
	abstract = {This paper uses a dataset of over 900,000 news stories to test whether news can predict stock returns. It ﬁnds that ﬁrms with no news have distinctly diﬀerent average future returns than ﬁrms with news. We measure sentiment with the Harvard psychosocial dictionary used by Tetlock, SaarTsechansky, and Macskassy (2008), the ﬁnancial dictionary of Loughran and McDonald (2011), and a proprietary Thomson-Reuters neural network. Simpler processing techniques predict short-term returns that are quickly reversed, while more sophisticated techniques predict larger and more persistent returns. Conﬁrming previous research, daily news predicts stock returns for only 1-2 days. But weekly news predicts stock returns for a quarter year. Positive news stories increase stock returns quickly, but negative stories have a long-delayed reaction.},
	language = {en},
	urldate = {2019-11-23},
	journal = {SSRN Electronic Journal},
	author = {Heston, Steven L. and Sinha, Nitish Ranjan},
	year = {2013},
	file = {Heston and Sinha - 2013 - News versus Sentiment Comparing Textual Processin.pdf:/Users/jkittler/Zotero/storage/95L275CD/Heston and Sinha - 2013 - News versus Sentiment Comparing Textual Processin.pdf:application/pdf}
}

@misc{noauthor_using_nodate,
	title = {Using {NLP} and {Deep} {Learning} to {Predict} {Stock} {Price} {Movements}},
	url = {https://towardsdatascience.com/using-nlp-and-deep-learning-to-predict-the-stock-market-64eb9229e102},
	urldate = {2019-11-23},
	file = {Using NLP and Deep Learning to Predict Stock Price Movements:/Users/jkittler/Zotero/storage/P3M8YY7G/using-nlp-and-deep-learning-to-predict-the-stock-market-64eb9229e102.html:text/html}
}

@article{saleh_neural_nodate,
	title = {Neural based event-driven stock rally prediction using {SEC} ﬁlings and {Twitter} data},
	abstract = {In this work we present a deep learning based methodology for predicting eventdriven stock rallies. We apply neural methods to the data presented in [1] as well as a dataset created for this work that includes tweets. We implement a combination of neural networks using both textual and non-textual features to predict stock movements on days of 8-K report ﬁlings. Our best model, on the dataset from [1] (Task 1) using GloVe embeddings with an LSTM and fully connected layers achieves an accuracy of 51\%. Our model on the generated dataset (Task 2) performs at a 52.61\% accuracy. In both tasks, we observe that adding textual information improves model performance. We also ﬁnd that the attention distribution over full tweets yields exciting qualitative results, giving higher scores to tweets with rich information content.},
	language = {en},
	author = {Saleh, Magdy and Nair, Surag},
	pages = {8},
	file = {Saleh and Nair - Neural based event-driven stock rally prediction u.pdf:/Users/jkittler/Zotero/storage/VUYY8KGX/Saleh and Nair - Neural based event-driven stock rally prediction u.pdf:application/pdf}
}

@misc{noauthor_sec.gov_nodate,
	title = {{SEC}.gov {\textbar} {What} {We} {Do}},
	url = {https://www.sec.gov/Article/whatwedo.html},
	urldate = {2019-11-23},
	file = {SEC.gov | What We Do:/Users/jkittler/Zotero/storage/2R6YXVYR/whatwedo.html:text/html}
}

@misc{noauthor_sec.gov_nodate-1,
	title = {{SEC}.gov {\textbar} {Form} 8-{K}},
	url = {https://www.sec.gov/fast-answers/answersform8khtm.html},
	urldate = {2019-11-23},
	file = {SEC.gov | Form 8-K:/Users/jkittler/Zotero/storage/N37R45YM/answersform8khtm.html:text/html}
}

@misc{kenton_8-k_nodate,
	title = {8-{K} ({Form} 8K)},
	url = {https://www.investopedia.com/terms/1/8-k.asp},
	abstract = {Companies are required by the Securities and Exchange Commission to file an 8-K to announce major events relevant to shareholders, such as an acquisition.},
	language = {en},
	urldate = {2019-11-23},
	journal = {Investopedia},
	author = {Kenton, Will},
	file = {Snapshot:/Users/jkittler/Zotero/storage/9U7WJXKK/8-k.html:text/html}
}

@misc{noauthor_alpaca_nodate,
	title = {Alpaca - {Commission}-{Free} {API} {First} {Stock} {Brokerage}},
	url = {https://alpaca.markets},
	abstract = {Alpaca API lets you build and trade with real-time market data for free.},
	language = {en},
	urldate = {2019-11-23},
	journal = {Alpaca},
	file = {Snapshot:/Users/jkittler/Zotero/storage/HUUK7MDU/alpaca.markets.html:text/html}
}

@article{gleason_selective_nodate,
	title = {Selective {Disclosure} and the {Role} of {Form} 8-{K} in the {Post}-{Reg} {FD} {Era}},
	abstract = {We investigate the impact of Form 8-K filings on cross-firm differences in analysts’ private or idiosyncratic information in the post-Reg FD era. Using firms’ connections to the investment community to identify the likelihood of selective disclosure, we document differences in analysts’ idiosyncratic information arising from selective disclosure before 8-K filings. While filings of 8-Ks pursuant to Reg FD attenuate the link between connections and analysts’ idiosyncratic information, they do so only after selective disclosures have already resulted in some analysts having better private information. In addition, the connections continue to facilitate private information search after the filings of non-Reg FD-specific 8-Ks.},
	language = {en},
	author = {Gleason, Cristi and Ling, Zhejia and Zhao, Rong},
	pages = {62},
	file = {Gleason et al. - Selective Disclosure and the Role of Form 8-K in t.pdf:/Users/jkittler/Zotero/storage/GD8ZSV3X/Gleason et al. - Selective Disclosure and the Role of Form 8-K in t.pdf:application/pdf}
}

@article{lerman_new_2010,
	title = {The new {Form} 8-{K} disclosures},
	volume = {15},
	issn = {1380-6653, 1573-7136},
	url = {http://link.springer.com/10.1007/s11142-009-9114-7},
	doi = {10.1007/s11142-009-9114-7},
	abstract = {The Securities and Exchange Commission (SEC) has mandated new disclosure requirements in Form 8-K, which became effective on August 23, 2004. The SEC expanded the list of items that have to be reported and accelerated the timeliness of these reports. This study examines the market reactions to 8-Ks filed under the new SEC regime and investigates whether periodic reports (10-K/Qs) became less informative under the new 8-K disclosure rules. We observe that the newly required 8-K items constitute over half of all filings and that most firms disclose the required items within the new shortened period (four business days). We find that all disclosed items (old and new) are associated with abnormal volume and return volatility around both the event and the SEC filing dates, and some items have significant return drifts after the SEC filings. Surprisingly, we find that the information content of periodic reports has not diminished by the more expansive and timely 8-K disclosures under the new guidance, possibly indicating that investors may use periodic filings to interpret the effects of material events that had been disclosed earlier.},
	language = {en},
	number = {4},
	urldate = {2019-11-28},
	journal = {Review of Accounting Studies},
	author = {Lerman, Alina and Livnat, Joshua},
	month = dec,
	year = {2010},
	pages = {752--778},
	file = {Lerman and Livnat - 2010 - The new Form 8-K disclosures.pdf:/Users/jkittler/Zotero/storage/SYXHPPAW/Lerman and Livnat - 2010 - The new Form 8-K disclosures.pdf:application/pdf}
}

@article{holowczak_testing_2019,
	title = {Testing market response to auditor change filings: {A} comparison of machine learning classifiers},
	volume = {5},
	issn = {24059188},
	shorttitle = {Testing market response to auditor change filings},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2405918818300096},
	doi = {10.1016/j.jfds.2018.08.001},
	abstract = {The use of textual information contained in company ﬁlings with the Securities Exchange Commission (SEC), including annual reports on Form 10-K, quarterly reports on Form 10-Q, and current reports on Form 8-K, has gained the increased attention of ﬁnance and accounting researchers. In this paper we use a set of machine learning methods to predict the market response to changes in a ﬁrm's auditor as reported in public ﬁlings. We vectorize the text of 8-K ﬁlings to test whether the resulting feature matrix can explain the sign of the market response to the ﬁling. Speciﬁcally, using classiﬁcation algorithms and a sample consisting of the Item 4.01 text of 8-K documents, which provides information on changes in auditors of companies that are registered with the SEC, we predict the sign of the cumulative abnormal return (CAR) around 8-K ﬁling dates. We report the correct classiﬁcation performance and time efﬁciency of the classiﬁcation algorithms. Our results show some improvement over the naïve classiﬁcation method.},
	language = {en},
	number = {1},
	urldate = {2019-11-28},
	journal = {The Journal of Finance and Data Science},
	author = {Holowczak, Richard and Louton, David and Saraoglu, Hakan},
	month = mar,
	year = {2019},
	pages = {48--59},
	file = {Holowczak et al. - 2019 - Testing market response to auditor change filings.pdf:/Users/jkittler/Zotero/storage/8JR858ZY/Holowczak et al. - 2019 - Testing market response to auditor change filings.pdf:application/pdf}
}

@article{dubrov_predicting_2015,
	title = {Predicting {Stock} {Returns} by {Automatically} {Analyzing} {Company} {News} {Announcements}},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=2684558},
	doi = {10.2139/ssrn.2684558},
	abstract = {We study the impact of company news announcements on stock prices and show that these announcements aﬀect daily excess returns. We demonstrate that daily excess return direction can be predicted with statistical signiﬁcance by analyzing news announcements, using automatic natural language processing tools. We use state-ofthe-art text analysis algorithms in order to automatically extract meaningful information from news announcements, thus expanding the set of tools available to researchers in the ﬁeld.},
	language = {en},
	urldate = {2019-11-28},
	journal = {SSRN Electronic Journal},
	author = {Dubrov, Bella},
	year = {2015},
	file = {Dubrov - 2015 - Predicting Stock Returns by Automatically Analyzin.pdf:/Users/jkittler/Zotero/storage/E3K3K626/Dubrov - 2015 - Predicting Stock Returns by Automatically Analyzin.pdf:application/pdf}
}

@article{shah_stock_2019,
	title = {Stock {Market} {Analysis}: {A} {Review} and {Taxonomy} of {Prediction} {Techniques}},
	volume = {7},
	issn = {2227-7072},
	shorttitle = {Stock {Market} {Analysis}},
	url = {https://www.mdpi.com/2227-7072/7/2/26},
	doi = {10.3390/ijfs7020026},
	abstract = {Stock market prediction has always caught the attention of many analysts and researchers. Popular theories suggest that stock markets are essentially a random walk and it is a fool’s game to try and predict them. Predicting stock prices is a challenging problem in itself because of the number of variables which are involved. In the short term, the market behaves like a voting machine but in the longer term, it acts like a weighing machine and hence there is scope for predicting the market movements for a longer timeframe. Application of machine learning techniques and other algorithms for stock price analysis and forecasting is an area that shows great promise. In this paper, we first provide a concise review of stock markets and taxonomy of stock market prediction methods. We then focus on some of the research achievements in stock analysis and prediction. We discuss technical, fundamental, short- and long-term approaches used for stock analysis. Finally, we present some challenges and research opportunities in this field.},
	language = {en},
	number = {2},
	urldate = {2019-11-28},
	journal = {International Journal of Financial Studies},
	author = {Shah, Dev and Isah, Haruna and Zulkernine, Farhana},
	month = may,
	year = {2019},
	pages = {26},
	file = {Shah et al. - 2019 - Stock Market Analysis A Review and Taxonomy of Pr.pdf:/Users/jkittler/Zotero/storage/MS9BU49V/Shah et al. - 2019 - Stock Market Analysis A Review and Taxonomy of Pr.pdf:application/pdf}
}

@misc{babbe_bert_2019,
	title = {{BERT} is the {Word}: {Predicting} {Stock} {Prices} with {Language} {Models}},
	shorttitle = {{BERT} is the {Word}},
	url = {https://medium.com/@babbemark/bert-is-the-word-predicting-stock-prices-with-language-models-8d5205b8537c},
	abstract = {Project By: Mark Babbe, Cory Nguyen, Won Lee, and Hanny Noueilaty},
	language = {en},
	urldate = {2019-11-28},
	journal = {Medium},
	author = {Babbe, Mark},
	month = mar,
	year = {2019},
	file = {Snapshot:/Users/jkittler/Zotero/storage/YM8JJLQ9/bert-is-the-word-predicting-stock-prices-with-language-models-8d5205b8537c.html:text/html}
}

@article{kraus_decision_2017,
	title = {Decision support from financial disclosures with deep neural networks and transfer learning},
	volume = {104},
	issn = {01679236},
	url = {http://arxiv.org/abs/1710.03954},
	doi = {10.1016/j.dss.2017.10.001},
	abstract = {Company disclosures greatly aid in the process of financial decision-making; therefore, they are consulted by financial investors and automated traders before exercising ownership in stocks. While humans are usually able to correctly interpret the content, the same is rarely true of computerized decision support systems, which struggle with the complexity and ambiguity of natural language. A possible remedy is represented by deep learning, which overcomes several shortcomings of traditional methods of text mining. For instance, recurrent neural networks, such as long short-term memories, employ hierarchical structures, together with a large number of hidden layers, to automatically extract features from ordered sequences of words and capture highly non-linear relationships such as context-dependent meanings. However, deep learning has only recently started to receive traction, possibly because its performance is largely untested. Hence, this paper studies the use of deep neural networks for financial decision support. We additionally experiment with transfer learning, in which we pre-train the network on a different corpus with a length of 139.1 million words. Our results reveal a higher directional accuracy as compared to traditional machine learning when predicting stock price movements in response to financial disclosures. Our work thereby helps to highlight the business value of deep learning and provides recommendations to practitioners and executives.},
	language = {en},
	urldate = {2019-11-28},
	journal = {Decision Support Systems},
	author = {Kraus, Mathias and Feuerriegel, Stefan},
	month = dec,
	year = {2017},
	note = {arXiv: 1710.03954},
	keywords = {Computer Science - Computation and Language},
	pages = {38--48},
	file = {Kraus and Feuerriegel - 2017 - Decision support from financial disclosures with d.pdf:/Users/jkittler/Zotero/storage/EQVTXN2Z/Kraus and Feuerriegel - 2017 - Decision support from financial disclosures with d.pdf:application/pdf}
}

@article{sakarwala_use_2019,
	title = {Use {Advances} in {Data} {Science} and {Computing} {Power} to {Invest} in {Stock} {Market}},
	volume = {2},
	abstract = {As part of its overseeing of capital markets, the Securities and Exchange Commission (SEC) requires ﬁrms with publicly traded shares to issue periodic reports to shareholders. These SEC ﬁlings are part of the SEC’s Electronic Data Gathering, Analysis, and Retrieval system (EDGAR), a large online database. Financial services and banking industry have armies of analysts that are dedicated to rushing over, analyzing, and attempting to quantify qualitative data from this SEC mandated reporting. We sought to prototype a predictive model to render consistent judgments on a company’s prospects, based on the written textual sections of public earnings releases extracted from SEC 8-K ﬁnancial reports and actual stock market performance. In this project, we leverage data from E DGAR to model the viability to predict the stock price through, natural language processing (NLP) and deep learning methods. The model used to predict the stock movement in the near futures (next few days from the release of report) by incorporating relevant ﬁnancial information, such as recent stock price movement and above or below earnings, and other textual information from these ﬁnancial reports. Our results will demonstrate how a deep learning model trained on text in SEC Document ﬁlings could provide a valuable signal to an investment decision maker. The results will be most important in 1-5 days (i.e. The next day after the ﬁnancial event) but persist(constant or in some trend up or down) for up to ﬁve days.},
	language = {en},
	number = {1},
	author = {Sakarwala, Mustafa A and Tanaydin, Anthony},
	year = {2019},
	pages = {20},
	file = {Sakarwala and Tanaydin - 2019 - Use Advances in Data Science and Computing Power t.pdf:/Users/jkittler/Zotero/storage/KVRB9BYZ/Sakarwala and Tanaydin - 2019 - Use Advances in Data Science and Computing Power t.pdf:application/pdf}
}

@article{doucette_content_nodate,
	title = {Content of {Annual} {Reports} as a {Predictor} for {Long} {Term} {Stock} {Price} {Movements}},
	abstract = {This paper examines the possibility of automatic extraction of future stock price information from the annual Form 10-K produced by publicly traded companies in the United States of America. While previous approaches to automatically interpreting corporate documents have tended to utilize extensive expert knowledge to preprocess and analyze documents, our approach inputs documents verbatim to a compression classiﬁer. We demonstrate the effectiveness of the new approach on a newly constructed dataset based around the Dow Jones Industrial Average over the period 1994-2009. We ﬁnd statistically signiﬁcant increase in average returns of stocks recommended by the new system as compared with the Dow as a whole. Also examined are two hypotheses regarding the predictive power of 10-K reports. First, whether congressional attempts to make Form 10-K ﬁlings more informative had a measurable impact, and second, whether the ﬁlings have long-term predictive value in a dynamically changing market.},
	language = {en},
	author = {Doucette, John A and Cohen, Robin},
	pages = {6},
	file = {Doucette and Cohen - Content of Annual Reports as a Predictor for Long .pdf:/Users/jkittler/Zotero/storage/VUL6VALR/Doucette and Cohen - Content of Annual Reports as a Predictor for Long .pdf:application/pdf}
}

@article{prokhorenkova_catboost:_2019,
	title = {{CatBoost}: unbiased boosting with categorical features},
	shorttitle = {{CatBoost}},
	url = {http://arxiv.org/abs/1706.09516},
	abstract = {This paper presents the key algorithmic techniques behind CatBoost, a new gradient boosting toolkit. Their combination leads to CatBoost outperforming other publicly available boosting implementations in terms of quality on a variety of datasets. Two critical algorithmic advances introduced in CatBoost are the implementation of ordered boosting, a permutation-driven alternative to the classic algorithm, and an innovative algorithm for processing categorical features. Both techniques were created to ﬁght a prediction shift caused by a special kind of target leakage present in all currently existing implementations of gradient boosting algorithms. In this paper, we provide a detailed analysis of this problem and demonstrate that proposed algorithms solve it effectively, leading to excellent empirical results.},
	language = {en},
	urldate = {2019-12-02},
	journal = {arXiv:1706.09516 [cs]},
	author = {Prokhorenkova, Liudmila and Gusev, Gleb and Vorobev, Aleksandr and Dorogush, Anna Veronika and Gulin, Andrey},
	month = jan,
	year = {2019},
	note = {arXiv: 1706.09516},
	keywords = {Computer Science - Machine Learning},
	file = {Prokhorenkova et al. - 2019 - CatBoost unbiased boosting with categorical featu.pdf:/Users/jkittler/Zotero/storage/FMZN4LN3/Prokhorenkova et al. - 2019 - CatBoost unbiased boosting with categorical featu.pdf:application/pdf}
}

@article{chen_xgboost:_2016,
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	shorttitle = {{XGBoost}},
	url = {http://arxiv.org/abs/1603.02754},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	language = {en},
	urldate = {2019-12-02},
	journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	note = {arXiv: 1603.02754},
	keywords = {Computer Science - Machine Learning},
	pages = {785--794},
	annote = {Comment: KDD'16 changed all figures to type1},
	file = {Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:/Users/jkittler/Zotero/storage/JI38X2WL/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf}
}

@inproceedings{ke_lightgbm:_2017,
	title = {Lightgbm: {A} highly efficient gradient boosting decision tree},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	year = {2017},
	pages = {3146--3154},
	file = {Ke et al. - LightGBM A Highly Efﬁcient Gradient Boosting Deci.pdf:/Users/jkittler/Zotero/storage/2Z57WXBF/Ke et al. - LightGBM A Highly Efﬁcient Gradient Boosting Deci.pdf:application/pdf}
}

@article{dorogush_catboost:_2018,
	title = {{CatBoost}: gradient boosting with categorical features support},
	journal = {arXiv preprint arXiv:1810.11363},
	author = {Dorogush, Anna Veronika and Ershov, Vasily and Gulin, Andrey},
	year = {2018},
	file = {Dorogush et al. - 2018 - CatBoost gradient boosting with categorical featu.pdf:/Users/jkittler/Zotero/storage/JP8SYP5B/Dorogush et al. - 2018 - CatBoost gradient boosting with categorical featu.pdf:application/pdf}
}

@misc{noauthor_multiclassification:_nodate,
	type = {concept},
	title = {Multiclassification: objectives and metrics - {CatBoost}. {Documentation}},
	shorttitle = {Multiclassification},
	url = {https://catboost.ai/docs/concepts/loss-functions-multiclassification.html},
	abstract = {Name Used for optimization User-defined parameters Formula and/or description MultiClass + use\_weights Default: true Calculation principles MultiClassOneVsAll + use\_weights Default: true Calculation principles Precision – use\_weights Default: true This function is calculated separately for each class k numbered from 0 to M – 1. Calculation principles Recall – use\_weights Default: true This function is calculated separately for each class k numbered from 0 to M – 1. Calculation principles F1 – use\_weights Default: true This function is calculated separately for each class k numbered from 0 to M – 1. Calculation principles TotalF1 – use\_weights Default: true Calculation principles MCC – use\_weights Default: true Calculation principles Accuracy – use\_weights Default: true Calculation principles HingeLoss – use\_weights Default: true See the Wikipedia article. HammingLoss – use\_weights Default: true Calculation principles ZeroOneLoss – use\_weights Default: true Calculation principles Kappa – – Calculation principles WKappa – – Calculation principles AUC* – use\_weights Default: false type Default: Mu misclass\_cost\_matrix Default: All non-diagonal matrix elements are set to 1. All diagonal elements (such that i = j) are set to 0. Calculation principles},
	language = {en},
	urldate = {2019-12-02},
	file = {Snapshot:/Users/jkittler/Zotero/storage/V5IKDSZP/loss-functions-multiclassification.html:text/html}
}

@misc{noauthor_module:_nodate,
	title = {Module: tf.losses  {\textbar}  {TensorFlow} {Core} r2.0},
	url = {https://www.tensorflow.org/api_docs/python/tf/losses},
	urldate = {2019-12-02},
	file = {Module\: tf.losses  |  TensorFlow Core r2.0:/Users/jkittler/Zotero/storage/ZUZZS6DU/losses.html:text/html}
}

@misc{noauthor_tf.keras.losses.categoricalcrossentropy_nodate,
	title = {tf.keras.losses.{CategoricalCrossentropy} {\textbar} {TensorFlow} {Core} r2.0},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy},
	language = {en},
	urldate = {2019-12-02},
	file = {Snapshot:/Users/jkittler/Zotero/storage/2V2KP4C8/CategoricalCrossentropy.html:text/html}
}