\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[margin=0.85in]{geometry}
\usepackage[backend=bibtex,style=ieee]{biblatex}
\usepackage{newunicodechar}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows}
\newunicodechar{ï¬}{fi}
\addbibresource{textmining_julki092.bib}
\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\newcommand{\comment}[1]{}

\begin{document}
	
	\title{\textsc{732A92 Text Mining} \\ [2.0cm]
		\HRule{0.5pt} \\
		\LARGE \textbf{\uppercase{Classifying Stock Price Movements based on 8-K SEC filings with Gradient Boosting Decision Trees (CatBoost)}}
		\HRule{2pt} \\ [0.5cm]
		\normalsize \today \vspace*{5\baselineskip}}
	
	\date{}
	
	\author{
		Name: Julius Kittler \\ 
		Student ID: julki092 \\ 
		Link\"{o}ping University}
	
	\maketitle
	\newpage
	
	\begin{abstract}
		
	This text mining project makes use of filings from the U.S. Securities and Exchange Commission (SEC), in particular 8-K filings, to forecast short-term percentage changes in stock prices. 8-K filings have to be published by public companies in the U.S. for certain defined, business-relevant events. The forecasting problem is approached as a classification problem with five classes: large decrease, small decrease, no change, small increase and large increase (of the stock price before and after filing date of the 8-K filing). Instead of using existing data sets, a new, up-to-date data set is created, with data for the year 2018 and the first quarter of 2019. Furthermore, this project makes a contribution by testing a new approach to this problem: gradient boosting decision trees (GBDT) with tf-idf bag-of-words. Previous research had found random forests to perform very well on this problem. Hence, the methodological choice for this project fell on GBDT as a more advanced ensemble model. Exclusively text data was used for training the models. The best model could achieve an additional 10\% accuracy on the test data compared to the majority classifier, which matches with previous findings from the literature.
	
	\end{abstract}

	\newpage
	\tableofcontents
	\newpage
	\listoffigures
	\listoftables
	\newpage

	\section{Introduction}
	
	Forecasting stock prices has been a relevant problem since the existence of publicly traded companies. Today, it is an even more relevant problem than ever before because we have the technological infrastructure to put our forecasts to practice in form of automatic trading systems. Not only can we obtain massive amounts of financial data via APIs, but we can also execute trades via APIs. With commission free trading becoming the industry standard in the U.S., executing trades via APIs is even offered for free by some companies nowadays \cite{noauthor_alpaca_nodate}. 
	
	The goal of this project is to classify discretized stock price percentage changes based on text data from SEC filings, in particular 8-K filings. SEC filings are official documents that publicly traded companies in the U.S. are required to publish to inform their shareholders about the business. The SEC, the U.S. Securities and Exchange Commission, is the governmental agency that enforces this \cite{noauthor_sec.gov_nodate-1}. While there are various different types of filings, e.g. for annual and quarterly reports, this project focusses exclusively on 8-K filings. Because 8-K filings correspond to major events for the company and because they need to be published shortly after an event occurred (within 4 days), 8-K filings seem interesting for predicting short-term volatility in the stock market. 
	
	Companies need to publish 8-K filings for major events such as a change in the board of directors, a potential delisting from a stock exchange or a merger. In total, there are 31 different 8-K filing events from 9 different sections. A complete list of all events and sections, taken from the official SEC website \cite{noauthor_sec.gov_nodate-1}, is shown in the table ~\ref{table:8kevents}. One 8-K filing document may contain information for several such events. For instance, one particular 8-K filing document may contain information about Item 5.02, Item 6.02 and Item 9.01 at the same time. Every 8-K filing clearly states for which events it contains information. To see some examples of 8-K filings, one may go to the official  \href{https://www.sec.gov/cgi-bin/browse-edgar?company=&CIK=&type=8-K&owner=include&count=40&action=getcurrent}{SEC website}. In particular, three examples can be found here: \href{https://www.sec.gov/ix?doc=/Archives/edgar/data/1339947/000119312519299728/d840037d8k.htm}{example 1}, \href{https://www.sec.gov/Archives/edgar/data/883975/000149315219018330/form8-k.htm}{example 2}, \href{https://www.sec.gov/Archives/edgar/data/1419275/000118518519001650/greenbox20191125_8k.htm}{example 3}.
	
	
	
	
	
	\begin{table}[h!]
	\centering
	\caption{Overview of all 8-K sections and events}
	\label{table:8kevents}
	
	\begin{tabularx}{\textwidth}{|X|l|X|}
		\toprule
		&      &                                   \\
		Section & Item & Event \\
		\midrule
		Registrant's Business and Operations & 1.01 & Entry into a Material Definitive Agreement \\
		& 1.02 & Termination of a Material Definitive Agreement \\
		& 1.03 & Bankruptcy or Receivership \\
		& 1.04 & Mine Safety - Reporting of Shutdowns and Patterns of Violations \\
		Financial Information & 2.01 & Completion of Acquisition or Disposition of Assets \\
		& 2.02 & Results of Operations and Financial Condition \\
		& 2.03 & Creation of a Direct Financial Obligation or an Obligation under an Off-Balance Sheet Arrangement of a Registrant \\
		& 2.04 & Triggering Events That Accelerate or Increase a Direct Financial Obligation or an Obligation under an Off-Balance Sheet Arrangement \\
		& 2.05 & Costs Associated with Exit or Disposal Activities \\
		& 2.06 & Material Impairments \\
		Securities and Trading Markets & 3.01 & Notice of Delisting or Failure to Satisfy a Continued Listing Rule or Standard; Transfer of Listing \\
		& 3.02 & Unregistered Sales of Equity Securities \\
		& 3.03 & Material Modification to Rights of Security Holders \\
		Matters Related to Accountants and Financial Statements & 4.01 & Changes in Registrant's Certifying Accountant \\
		& 4.02 & Non-Reliance on Previously Issued Financial Statements or a Related Audit Report or Completed Interim Review \\
		Corporate Governance and Management & 5.01 & Changes in Control of Registrant \\
		& 5.02 & Departure of Directors or Certain Officers; Election of Directors; Appointment of Certain Officers; Compensatory Arrangements of Certain Officers \\
		& 5.03 & Amendments to Articles of Incorporation or Bylaws; Change in Fiscal Year \\
		& 5.04 & Temporary Suspension of Trading Under Registrant's Employee Benefit Plans \\
		& 5.05 & Amendment to Registrant's Code of Ethics, or Waiver of a Provision of the Code of Ethics \\
		& 5.06 & Change in Shell Company Status \\
		& 5.07 & Submission of Matters to a Vote of Security Holders \\
		& 5.08 & Shareholder Director Nominations \\
		Asset-Backed Securities & 6.01 & ABS Informational and Computational Material \\
		& 6.02 & Change of Servicer or Trustee \\
		& 6.03 & Change in Credit Enhancement or Other External Support \\
		& 6.04 & Failure to Make a Required Distribution \\
		& 6.05 & Securities Act Updating Disclosure \\
		Regulation FD & 7.01 & Regulation FD Disclosure \\
		Other Events & 8.01 & Other Events \\
		Financial Statements and Exhibits & 9.01 & Financial Statements and Exhibits \\
		\bottomrule
	\end{tabularx}

	\end{table}%	



	\section{Theory}
	
	\subsection{Stock Price Forecasting with 8-K filings}
	
	This section gives a short overview of previous work about forecasting stock prices with 8-K filings. Since the focus of this project is classification, this literature review also focuses on previous work with classification. There are few public research papers that made use of 8-K filings for stock price forecasts. However, the papers that are publicly available show promising results, with between 4\% and 10\% additional accuracy achieved by adding text features from 8-K filings to the classification models.
	
	For instance, Lee et al. could achieve an increase in accuracy by 10\% when including text data from 8-K filings into a baseline model that only used financial metrics \cite{lee_importance_2014}. This study was solving a classification task with three classes: price increase ($> 1\%$), price decrease ($< -1\%$) or no relevant change ($< |1 \%|$). Compared to a random classification with 33\% accuracy and a majority-class classification of 35\% accuracy, the best model of the study could achieve a 55\% accuracy on the test data. The main model used for this study was a random forest classifier, which outperformed other models such as multi layer perceptron and logistic regression. Unigram features of the text data were used along with non-negative matrix factorization for dimensionality reduction.
	
	Another study by Saleh et al. extended the research by Lee et al. \cite{saleh_neural_nodate}. In addition to the data from the 8-K filings, the researchers used text data from Twitter. Furthermore, they used convolutional neural networks (CNNs) and recurrent neural networks (RNNs) instead of random forests. Compared to a majority-class classification of 42\% without twitter data and 49\% with twitter data, best test accuracies were 51\% and 53\% respectively. Instead of unigram features, word embeddings from GloVe were used.
	
	Holowczak et al. compared the performance of various common algorithms (random forest, naive bayes, support vector machine, k-nearest neighbor and ridge classifier) on a binay classification task (price increase vs. decrease) \cite{holowczak_testing_2019}. However, only 8-K filings for a particular event, item 4.01, were used. The best results were achieved with a linear support vector classifier with a classification accuracy of 54.4\% on the test data (compared to a 50\% accuracy by a majority classifier). The text data had been transformed with frequency-inverse document frequency (tf-id) vectorization.
	
	
	\subsection{Gradient Boosting Decision Trees (GBDT) in General}
	
	Gradient boosting with decision trees became popular with the development of XGBoost in 2016 \cite{chen_xgboost:_2016} and lightGBM in 2017 \cite{ke_lightgbm:_2017}. In 2018, another GBDT library called CatBoost was published, which claims to handle categorical features particularly well. Furthermore, CatBoost was shown to outperform XGBoost and lightGBM on various datasets with default parameters. For this reason, CatBoost is used in this project. The implementational details of CatBoost are rather involved and are therefore not fully covered in this report. Instead, what follows is a short introduction to GBDT. Differences betweeen CatBoost and other implementations of GBDT are mainly that CatBoost handles categorical features very well and that it uses unbiased gradient estimates \cite{dorogush_catboost:_2018}. 
	
	In GBDT, several concepts commonly used in machine learning are combined: gradients, boosting and decision trees. The main idea of \textbf{boosting} is to iteratively fit simplistic, additive models (so-called weak models) such that each model reduces the errors made by the ensemble model in the corresponding previous iteration. This can be achieved by fitting each model directly on the residuals of the ensemble model in the previous iteration. Gradient boosting generalizes this approach: each weak model is fitted on the \textbf{gradient} of the residuals from the previous iteration  \cite{chen_xgboost:_2016} \cite{friedman_greedy_2001}  \cite{noauthor_kaggle_nodate}, where the term residuals refers to a differentiable loss function.
	
	Recall that the gradient takes a scalar function (returning 1 scalar value given $P$ parameters) and returns a vector  (with $P$ values, each of them the partial derivative of the scalar function w.r.t. the $p$th parameter). In our case, the scalar function is the differentiable loss function $L^{(t-1)}$:

	\begin{align}
	\mathrm{L^{(t-1)}} = \sum_{i=1}^n l(y_i, \hat{y}_i^{(t-1)}), 
	\end{align} 
	
	where $i = 1, ..., N$ is the index for the training observations, $\hat{y_i}^{(t-1)}$ is the prediction for the $i$th training observation based on the ensemble model from the previous iteration and $l(y_i, \hat{y}_i^{(t-1)})$ is the loss for the $i$th training observation. If we take the gradient with respect to $\hat{y}_i^{(t-1)}$ as parameters, we get the following:
	
	\begin{align}
		\nabla L^{(t-1)} = \left[\frac{\partial l(y_1, \hat{y}_1^{(t-1)})}{\partial \hat{y}_1^{(t-1)}}, \frac{\partial l(y_2, \hat{y}_2^{(t-1)})}{\partial \hat{y}_2^{(t-1)}}, ..., \frac{\partial l(y_N, \hat{y}_N^{(t-1)})}{\partial \hat{y}_N^{(t-1)}}\right]
	\end{align}
	
	This is a row vector with N elements, where each element corresponds to the derivative of the loss of the $i$th training observation with respect to its prediction $\hat{y_i}^{(t-1)}$. For instance, if we used the squared error loss function, $l(y_i, \hat{y}_i^{(t-1)}) = \frac{1}{2} \left(y_i - \hat{y}_i^{(t-1)}\right)^2$, the elements in the gradient vector would be $\frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}} = -\left(y_i - \hat{y}_i^{(t-1)}\right) = \hat{y}_i^{(t-1)} - y_i$. Since the gradients here are absolute residuals, this corresponds to the simplest case mentioned above: fitting a weak model directly on the residuals of the ensemble model in the previous iteration. Importantly, we are not restricted to the squared error loss function. Instead, we can use any other differentiable loss function.
	
	In every iteration $t$, we would like to add a weak model that helps to decrease the loss from the previous iteration $L^{(t-1)}$. Since the gradient points into the direction of the maximum increase of the loss function $L^{(t-1)}$ but we would like to decrease the loss, we consider the negative gradient $-L^{(t-1)}$.
	We start by initializing $\hat{y}_i^{(0)}$ as a constant for all training observations $i$, for instance as the mean of $y$ from the training data. In every subsequent iteration $t$, we fit a weak model that predicts the negative gradient. We also learn a parameter that determines the step size towards the negative gradient. When we then add this model to the ensemble classifier, we essentially perform a step of gradient descent (not as usual by changing some parameters $w$, but instead by adding another weak model to an ensemble classifier). The prediction of the ensemble model for any observation $i$ can be represented as:
	
	\begin{align}
	 \hat{y}_i = \sum_{t = 0}^T \hat{y}_i^{(t)}, 
	\end{align}
	
	where $T$ is the number of iterations, i.e. weak models. Note that $\hat{y}_i$ may not be the final output of the model. For classification, we might pass $\hat{y}_i$ as input to a softmax function to get the predicted class for observation $i$.
	
	Finally, note that different types of weak models can be used with gradient boosting. However, in GBDT specifically, every weak model is a \textbf{decision tree}. Decision trees are tree-shaped models that consist of nodes connected with branches. To make a prediction, for an observation $i$, its feature vector $\textbf{x}_i$ is passed from the root node through the tree until it lands in a leaf node. For all vectors $\textbf{x}_i$ in the same leaf node, the prediction is made. At each node within the tree, a decision is made about the branch along which the $\textbf{x}_i$ is to be passed further. The decision is made based on a threshold $\tau_n$ for the value $\textbf{x}_{i}^{(j)}$, where $j$ is the index of the variable used at the threshold and $n$ is the index of the node. Among others, the parameters that need to be learned are the thresholds $\tau_n$, the variable indices $j$ used at the thresholds and depending on the implementation also the structure of the tree (e.g. number of nodes, depth of the tree).
	
	\section{Data}
	
	\subsection{Sources}
	
	The data used for this text mining project comes from a variety of sources. The stock price data (with daily resolution) was retrieved with the financial API \href{https://www.tiingo.com}{Tiingo}. The SEC filings were downloaded from \href{https://www.sec.gov/Archives/edgar/full-index/}{EDGAR}, the official archive for SEC filings. The overview of companies by CIK, necessary to merge the SEC filings with the stock prices, was taken from the service \href{http://rankandfiled.com/#/data/tickers}{Ranked and Filed}. Lastly, the overview of the 8-K events, used to extract the 8-K events from each filing, was taken from the  \href{https://www.sec.gov/fast-answers/answersform8khtm.html}{SEC documentation}. 
	
	\subsection{Retrieval}
	
	The dataset used for training the models was created with the following steps: 
	
	\begin{enumerate}
		\item \textbf{Getting 8-K filing overview by CIK:} A list of all 8-K filings for all quarters of 2018 and for the first quarter of 2019 was retrieved from EDGAR. This list contained a total of 76782 8-K filings, including the CIK number (to identify the company) and the filing date.
		\item \textbf{Getting 8-K filings overview by ticker symbol:} The list was merged with the data from Ranked and Filed to get the exchange market and ticker symbol of the companies corresponding to each 8-K filing.
		\item \textbf{Filtering out exchanges:} All  8-K filings from companies that were not listed on the NASDAQ, NYSE or AMEX (according to Ranked and Filed) were removed. In particular, the removed 8-K filings corresponded to companies listed on ARCA, OTC or OTCBB and to companies for which no information about the exchange market was available. The resulting list contained a total of 49070 8-K filings.
		\item \textbf{Filtering out missing stock prices:} All 8-K filings, for which no stock price data was available from Tiingo were removed from the list. The resulting shortlist contained a total of 45262 8-K filings. After further removing 8-K filings that had a stock price split before, on or after the day of the filing date, the shortlist contained 42087  8-K filings.
		\item \textbf{Adding stock prices by ticker symbol:} For the remaining 8-K filings, the stock price data was extracted from the Tiingo data, making use of the filing date from EDGAR. For each filing, the percentage change from the closing price of the day before the filing date and the open price of the day after the filing date was computed as target variable.
		\item \textbf{Downloading 8-K filings:} All 8-K filings from the shortlist were successfully downloaded from EDGAR as text documents with HTML format.
		\item \textbf{Filtering out during pre-processing:} When processing the data (see below) 8-K filings with more than 1 Mio. characters were removed due to file size limitations of the libraries that were used for processing. Furthermore, two outliers with a percentage change of approx. $2900$\% were removed. This left a total of 41763 SEC filings for the project.

		
	\end{enumerate}
	
	\subsection{Pre-Processing}
	
	\subsubsection{Text in General}
	
	Each raw 8-K filing, a text file containing HTML code, was processed as follows. First, graphics and embedded PDFs were removed and HTML tags were removed as well. Second, the resulting text data was tokenized with the natural language processing library spaCy, using the English language model \lstinline{en_core_web_sm}. Third, stop words, non-alphabetical tokens and tokens with only one character were removed. Fourth, the remaining tokens were lemmatized with spaCy. Fifth, the remaining tokens were all converted to lower case text.
	
	\subsubsection{Extraction of 8-K Events}
	
	The 8-K item events corresponding to each 8-K filing were extracted for analysis. This was done by searching all filing documents for all item identifiers. E.g. each document was searched for "Item 1.01", "Item 1.02", ..., "Item 9.01". Since these identifiers must occur in the titles of the 8-K filings that contain information about the respective events, they could be extracted by a simple search. 
	
	\subsubsection{Negation Encoding}
	
	For one experiment, words from negated parts of a sentence were encoded with the suffix \lstinline{_neg}. This was done by locating the strings not, 't, never, no, neither, nor, nobody, noone, nothing, nowhere, cannot in each sentence and adding said suffix to all words between such a string and the next punctuation (dot, comma, semicolon etc.). 
	

	\subsection{Description}
	
	
	\subsubsection{Target Variable Before Discretization}
	
	For each 8-K filing, the target variable is computed as the percentage change from the closing price of the day before the filing date to the opening price of the day after the filing date. Only Monday-Friday are considered, i.e. when the filing date is a Monday, then the closing price from the Friday before is used (instead of Sunday). 
	
	The percentage changes are normalized with the S\&P 500, a stock price index based on the 500 largest companies in the U.S. The normalization was done by subtracting the S\&P 500 percentage change from the stock price percentage change. For example, when the percentage change of the stock price corresponding to an 8-K filing was +5\% and when the percentage change of the S\&P 500 in the same period was +3\%, then the normalized percentage change is +2\%.
	
	Table ~\ref{table:stats_by_data} shows descriptive statistics for the percentage changes before discretization. The corresponding distribution is visualized in figure ~\ref{fig:dist_target_nondisc}. Note that visually, the distributions for training, test and validation set seem very similar, and hence the distribution was only plotted for the overall data. The distributions for the different data sets differ slightly, however, in their maximum values because the total number of data points with large percentage changes is relatively small. 

	
	\begin{table}[h!]
		\centering
		\caption{Descriptive statistics for percentage change by data type}
		\label{table:stats_by_data}
		
		\begin{tabular}{lrrrrrrrr}
			\toprule
			Data Type &     size &     min &     max &    mean &  percentile\_25 &  percentile\_50 &  percentile\_75 &     std \\
			\midrule
			training   &  27030.0 & -0.9386 &  4.9105 &  0.0011 &        -0.0158 &         0.0001 &         0.0165 &  0.0765 \\
			validation &   6758.0 & -0.9206 &  3.0863 &  0.0005 &        -0.0159 &         0.0004 &         0.0167 &  0.0726 \\
			test       &   7975.0 & -0.8135 &  2.8866 &  0.0021 &        -0.0163 &        -0.0001 &         0.0157 &  0.0759 \\
			total      &  41763.0 & -0.9386 &  4.9105 &  0.0012 &        -0.0159 &         0.0001 &         0.0164 &  0.0758 \\
			\bottomrule
		\end{tabular}
		
	\end{table}%	
	
	
	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{img/dist_target_nondisc.png}
		\caption{Distribution of the target variable before discretization}
		\label{fig:dist_target_nondisc}
	\end{figure}

	\subsubsection{Target Variable After Discretization}
		
	The normalized percentage changes were discretized into five bins of equal size, representing the five classes that are to be predicted in this project. Discretization into bins of equal size was used as discretization method to ensure balanced classes without having to over- or undersample. Five bins were chosen to distinguish between large, small and negligible percentage changes. The five classes correspond to large percentage decrease (\lstinline{lg_dec}), small percentage decrease (\lstinline{sm_dec}), no relevant change (\lstinline{same}), small percentage increase (\lstinline{sm_inc}) and large percentage increase (\lstinline{lg_inc}).
	
	Table ~\ref{table:stats_by_target} shows descriptive statistics for the percentage changes after discretization. This table basically illustrates what the classes used in this project actually represent. For instance, the class  \lstinline{lg_dec} represents any percentage change in the interval $[-0.9386, -0.0217]$. Here, $-0.9386$ represents $-93.86$\%. Figure ~\ref{table:stats_by_target} shows the number of observations by training, validation and test data. Due to the discretization method, the classes are very balanced, i.e. they occur similarly often, in the overall data as well as across the different data sets.
	
	\begin{table}[h!]
		\centering
		\caption{Descriptive statistics for percentage change by discretization level}
		\label{table:stats_by_target}
	
		\begin{tabular}{lrrrrrr}
			\toprule
			Disc. Level &  size &     min &     max &    mean &  median &     std \\
			\midrule
			lg\_dec &  8355 & -0.9386 & -0.0217 & -0.0681 & -0.0451 &  0.0687 \\
			sm\_dec &  8386 & -0.0217 & -0.0051 & -0.0121 & -0.0116 &  0.0047 \\
			same   &  8365 & -0.0051 &  0.0054 &  0.0002 &  0.0002 &  0.0030 \\
			sm\_inc &  8315 &  0.0054 &  0.0226 &  0.0126 &  0.0119 &  0.0048 \\
			lg\_inc &  8342 &  0.0227 &  4.9105 &  0.0736 &  0.0473 &  0.1166 \\
			\bottomrule
		\end{tabular}
	\end{table}%		


	\begin{figure}[h!]
	\includegraphics[width=\linewidth]{img/dist_target_disc.png}
	\caption{Distribution of the target variable after discretization}
	\label{fig:dist_target_disc}
	\end{figure}


	\subsubsection{8-K Filings}
	
	The distribution of the number of tokens per document is shown in figure ~\ref{fig:dist_num_tokens}. There are few documents with more than 10000 tokens. The largest 8-K filing has approximately 75000 tokens. Note that any filings with more than 1 Mio. characters have been filtered out before during pre-processing.

	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{img/dist_num_tokens.png}
		\caption{Distribution of the number of tokens per document}
		\label{fig:dist_num_tokens}
	\end{figure}

	The number of occurences of each 8-K event is displayed in figure ~\ref{fig:dist_8k_events}. Note that one 8-K filing can contain information about more than one event. Therefore, the total number of events, 89976, is more than twice as large as the total number of 8-K filings. The five most frequent events are: 9.01 (financial statements and exhibits), 2.02 (results of operations and financial condition), 8.01 (other events), 7.01 (regulation FD disclosure) and 5.02 (Departure of directors or certain officers; election of directors; appointment of certain officers; compensatory arrangements of certain officers).
	
	In figure ~\ref{fig:dist_class_by_8k}, the number of occurences of each 8-K event is again displayed, this time however with additional information about the corresponding discretized percentage change. For instance, one can see that there were approx. 7000 8-K filings with information about "Item 7.01" that were followed by a large \textit{decrease} in the stock price (dark orange) and another 7000 8-K filings with information about "Item 7.01" that were followed by a large \textit{increase} in the stock price (dark grey). Importantly, one can see that the same 8-K event, e.g. "Item 7.01", generally corresponds to an equally large number of stock price increases and decreases. In other words, whether a particular 8-K event contributes to a stock price increase or decrease does not seem to depend on the event itself but rather on the actual content of the 8-K filing about this event.
	

	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{img/dist_8k_events.png}
		\caption{Number of occurences of 8-K events}
		\label{fig:dist_8k_events}
	\end{figure}


	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{img/dist_class_by_8k.png}
		\caption{Number of occurences of classes by 8-K event}
		\label{fig:dist_class_by_8k}
	\end{figure}


	\section{Method}
	
	\subsection{Training, Validation, Testing}
	
	The available data covers the complete year 2018 and the first quarter of 2019, i.e. five quarters in total. The four quarters of 2018 were used as training and validation data and the first quarter of 2019 was used as test data. The data for 2018 was split randomly into 80\% training and 20\% validation data. 
	
	The training data was used to train the models (with and without cross validation). In particular, cross validation with three folds based on the training data was used when doing grid search for hyperparameter optimization. The baseline models were trained on the training data without cross validation. After training, all models were evaluated on the validation data. Based on the performance on the validation data, the best model was chosen. This model was then evaluated on the test data to obtain a generalization error and evaluate whether we could use the model in practice. 
	
	\subsection{Evaluation Metrics}
	
	For evaluating the results, accuracy, as well as avg. precision, recall and f1-score are reported. Accuracy was used as main criterion to determine the best model. When interpreting the performance of the best model on the test data, the focus was on the confusion matrix and on class-wise precision, an important metric for considering potential trading strategies based on the model.
	
	Class-wise precision is computed for each class individually. For a given class c, the class-wise precision is defined as the number of observations of class c that were classified as class c (TP for true positive), divided by the number of any observations that were classified as class c, regardless of whether they are of class c or not (TP + FP for the sum of all true positives and false positives).
	
	\begin{align}
	\mathrm{precision}_c = \frac{TP_c}{FP_c + TP_c}
	\end{align}
	
	Using precision as prioritized evaluation metric for the final model reflects the assumptions that a) we want to avoid loosing money in a trade and that b) missing out on a good trade is acceptable. To explain this, it is best to consider an example. Assume that we take a long position whenever the model classifies a large stock price increase (\verb|lg_inc|), i.e. we buy stocks expecting that their stock price will increase. If the stock price decreases instead, we would loose money. To avoid this loss, we ideally want that among all cases where the model classifies a large increase (FP + TP), the stock price actually increases (TP). This means that we care about the precision. 
	
	\subsection{Loss Function}
	
	For multi-class classification problems, the softmax function is generally used as loss function. Hence, it is also used for this project. However, the chosen loss function differs slightly from the softmax function because in GBDT, each observation $i$ is weighted with $w_i$. The loss function used for CatBoost is called \lstinline{MultiClass} loss function and is essentially a weighted softmax function \cite{noauthor_multiclassification:_nodate}. It is computed as a scalar value across all observations and classes. The definition is as follows:
	
	\begin{align}
	L = \frac{\sum_{i = 1}^N w_i \log \left(\frac{\exp a_{i, t_i}}{\sum_{j = 0}^{M-1} \exp a_{i, j}}\right)}{\sum_{i = 1}^N w_i}, 
	\end{align}
	
	where $i = 1, ..., N$ is the index for the observation, $j = 0, ..., M-1$ is the index of the class, $a_{i, j}$ is the model output for observation $i$ and class $j$, $w_i$ is the weight assigned for observation $i$ during training, and $t_i$ is the index of the true class of observation $i$ so that $a_{i, t_i}$ is the model output for the true class of observation $i$.
	
	Essentially, the term in the parentheses is the softmax function, which returns a value between 0 and 1 for each observation $i$ to belong to its true class $t_i$. If the model performs well for observation $i$, then the softmax value will be large. For instance, if the value is 1, this means that according to the model, we have a probability of 1 that observation $i$ is from class $t_i$, i.e. from the true class of the observation. In other words, the model would perfectly identify the true class of observation $i$. Since the log is taken, and since $log(1)=0$, the loss incurred for this perfectly classified observation will be 0. If the softmax value is smaller than 1, the loss for an observation will be negative and will hence affect the training process.
	
	\subsection{Experiment Description}
	
	\subsubsection{Baseline}
	
	Four commonly used models were included as baseline models. The data was processed in the same way for all these models: the \lstinline{CountVectorizer} from sklearn was used. \lstinline{min_df} was set to 0.001 and \lstinline{max_df} was set to 0.9, i.e. only tokens that were in fewer than 90\% of the documents and in more than 0.1\% of the documents were included. These parameter values were identified during a grid search experiment with CatBoost and then used for the baseline models as well.
	
	In addition to the baseline models, the majority classifier and the random guess classifier were reported. Due to balanced classes, the values of these two classifiers were the same.
	
	\subsubsection{GBDT}
	
	Various experiments and parameter searches were performed with CatBoost. Due to long run-times, an exhaustive grid search was not possible. Instead, a mixture of simple comparisons, grid search, and random search was used. Since experiments depended on each other, some results are reported here as well. Only the final model was evaluated on the validation data. For training and parameter tuning only the training data was used, with a simple percentage split (for step 1-2) and with 3-fold cross validation (for step 3-4).
	
	
	\begin{enumerate}
		\item 	A comparison between the \textbf{\lstinline{CountVectorizer} and the \lstinline{TfidfVectorizer}} was conducted (with 67\% of the training data for training and 33\% of the training data for validation). \lstinline{min_df} was set to a small number, 0.001, so that the vectorized data could fit in memory. CatBoost was used with default parameters. Since accuracy, as well as avg. precision and recall were better by 0.01 with the \lstinline{TfidfVectorizer}, only the \lstinline{TfidfVectorizer} was used in subsequent experiments.
		\item 	A comparison between text data \textbf{with and without negation encoding} was conducted (with 67\% of the training data for training and 33\% of the training data for validation). The default CatBoost classifier with \lstinline{TfidfVectorizer} was used. Since adding the negation encoding changed the evaluation metrics by only $<0.01$, the data without negation encoding was used in subsequent experiments for simplicity.
		\item 	A grid search for the \lstinline{TfidfVectorizer} parameters \textbf{\lstinline{min_df} and \lstinline{max_df}} was conducted. The values 0.001, 0.005, 0.01 and 0.85, 0.9, 0.95 were considered	respectively. The best parameters, 0.001 for \lstinline{min_df} and 0.9 for \lstinline{max_df}, were used in subsequent experiments. Note that the chosen parameter value for \lstinline{min_df} was the smallest among the tested values. However, it was not decreased further due to memory limitations.
		\item 	A random search (with 15 iterations) was done for the \textbf{CatBoost hyperparameters} \lstinline{learning_rate} (0.015, 0.02, 0.025, 0.03, 0.035), \lstinline{depth} (5, 6, 7, 8, 9), and \lstinline{iterations} (750, 800, 850, 900, 950). The best parameters were a \lstinline{depth} of 8, a number of \lstinline{iterations} of 900 and a \lstinline{learning_rate} of 0.03.
	\end{enumerate}
	
	In summary, the final CatBoost model was a CatBoost classifier with \lstinline{depth} of 8, 900 \lstinline{iterations} and a \lstinline{learning_rate} of 0.03. The \lstinline{TfidfVectorizer} was used with parameters 0.001 for \lstinline{min_df} and 0.9 for \lstinline{max_df}. Only unigrams were used.
	

	\subsection{Best Model Evaluation}
	
	In addition to class-wise precision and the confusion matrix, a backtesting with simplified assumptions was conducted to evaluate the final model on the test data from QTR1 2019. The backtesting was done with the following simplifying assumptions. Note that the max. loss per trade reflects a stop-loss, i.e. we would get out of a trade when reaching a 5\% loss on a specific trade (e.g. by selling all shares in case of a long position). The max. percentage change realized in case of profit is only set to 50\% because parts of the price increase mig
	ht happen before we might enter a trade (in pre- or after-market hours).
	
	\begin{enumerate}
		\item 	Amount of money placed on each trade: \$2K
		\item 	Cash in portfolio: enough to place any trade
		\item 	Commission per trade: \$0
		\item 	Max. loss of per trade: 5\% (i.e. \$100)
		\item 	Max. percentage change realized in case of profit: 50\%
	\end{enumerate}
	
	The backtesting was conducted in two variations. First, a trade was placed whenever the model classified \lstinline{lg_inc}. Second, a trade was placed whenever the model classified \lstinline{lg_inc} with a probability larger than 40\%. The threshold of 40\% was determined as a reasonable threshold based on the distribution of the predicted probabilities of the classified classes of the test data in figure ~\ref{fig:dist_pred_prob}.
	
	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{img/dist_pred_prob.png}
		\caption{Distribution of predicted probabilities of classified classes}
		\label{fig:dist_pred_prob}
	\end{figure}
	
	
	\section{Results}
	
	\subsection{Model Comparison}

	The performance of all classifiers on the training and validation data can be found in table ~\ref{table:results_train_valid}. The best model on the validation data was the CatBoost classifier with 11\% additional accuracy compared to a majority classifier and evaluation metrics ranging from 0.30 to 0.31. The CatBoost classifier is followed  by logistic regression which performs between 0.01 and 0.02 worse on the evaluation metrics. 

	\begin{table}[h!]
		\centering
		\caption{Classifier performance on the training and validation data}
		\label{table:results_train_valid}
		
		\begin{tabular}{lrrrrrrrr}
			\toprule
			Data & Model &     accuracy &     precision &     recall &    f1 \\
			\midrule
			& Random Guess   &  0.20 &  0.20 &   0.20 &  0.20 \\
			& Majority Class &    0.20 &  0.20 &   0.20 &   0.20 \\
			& Naive Bayes       &   0.31 & 0.30 &  0.31 &  0.29 \\
			Training & SVM      &  0.43 & 0.44 & 0.43 &  0.42 \\
			& Random Forrest      &  0.98 & 0.98 & 0.98 &  0.98 \\
			& Logistic Regression   &  0.42 & 0.43 & 0.42 &  0.42 \\
			& CatBoost (Final)   &  0.62 & 0.63 &  0.62 & 0.62 \\
			\midrule
			& Random Guess   &  0.20 &  0.20 &   0.20 &  0.20 \\
			& Majority Class &    0.20 &  0.20 &   0.20 &   0.20 \\
			& Naive Bayes       &   0.25 & 0.25 &  0.25 &  0.23 \\
			Validation & SVM      &  0.26 & 0.26 & 0.26 & 0.25 \\
			& Random Forrest      &  0.28 & 0.27 & 0.28 & 0.27 \\
			& Logistic Regression   &  0.29 & 0.29 &  0.29 &  0.28 \\
			& \textbf{CatBoost (Final)}  &  \textbf{0.31} & \textbf{0.30} & \textbf{ 0.31} & \textbf{0.30} \\
			\bottomrule
		\end{tabular}
		
	\end{table}%	

	\subsection{Best Model}
	
	\subsubsection{Evaluation Metrics}

	The performance of the final CatBoost classifier on the test data from QTR1 2019 is displayed in table ~\ref{table:results_test}. The corresponding confusion matrix with class-wise precision is shown in table ~\ref{table:best_model_cm_abs}. The performance on the test data is very similar to the performance on the validation data (with differences of only 0.00-0.01). The largest precision is achieved for the class \lstinline{lg_inc}, with a value of 0.36. This means that when classifying \lstinline{lg_inc}, the model is correct 36\% of the time. The model makes rather many mistakes when classifying \lstinline{sm_inc} and \lstinline{sm_dec}.

	\begin{table}[h!]
	\centering
	\caption{Classifier performance on the test data}
	\label{table:results_test}
	
	\begin{tabular}{lrrrrrrrr}
		\toprule
		Model &     accuracy &     precision &     recall &    f1 \\
		\midrule
		CatBoost (Final)   &  0.30 &  0.30 &   0.30 &  0.30 \\
		\bottomrule
	\end{tabular}
	
	\end{table}%


	\begin{table}[h!]
	\centering
	\caption{Confusion matrix of best model evaluated on test data}
	\label{table:best_model_cm_abs}
	
	\begin{tabular}{llrrrrr}
		\toprule
		&        & \multicolumn{5}{l}{Predicted} \\
		&        &    lg\_dec & sm\_dec & same & sm\_inc & lg\_inc \\
		\midrule
		True & lg\_dec &       496 &    248 &  235 &    226 &    435 \\
		& sm\_dec &       200 &    281 &  542 &    340 &    220 \\
		& same &       136 &    293 &  688 &    357 &    161 \\
		& sm\_inc &       200 &    258 &  513 &    335 &    218 \\
		& lg\_inc &       409 &    187 &  204 &    200 &    593 \\
\bottomrule
		Precision &  &    0.34 &  0.23 &  0.32 &   0.22 &    0.36 \\
		\bottomrule
	\end{tabular}

	\end{table}%

	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{img/class_by_lg_dec.png}
		\caption{Number of true classes among 8-K filings classified as \lstinline{lg_inc} by 8-K event}
		\label{fig:class_by_lg_dec}
	\end{figure}

 	Since \lstinline{lg_inc} has the highest precision, figure ~\ref{fig:class_by_lg_dec} shows the number of true classes among all observations that were classified as \lstinline{lg_inc} by 8-K event. E.g. approx. 570 of the 9.01 events classified as \lstinline{lg_inc} had the true class \lstinline{lg_inc} but approx. 440 of the 9.01 events classified as \lstinline{lg_inc} had the true class \lstinline{lg_dec}. There are only slight differences in precision across the different 8-K events. However, for events with little available data (e.g. 2.01, 2.03 and 3.02) the number of correct classifications is as large as the number of false classifications.

	\subsubsection{Backtesting}

	The backtesting results are displayed in table ~\ref{table:results_trading} and the cumulative profits across trades are shown in the figures ~\ref{fig:trade_lg_inc_all} and ~\ref{fig:trade_lg_inc_0.4}. The figures also show a random trader in comparison, which randomly places one trade for each trade of the non-random trader. Any 8-K filings from the corresponding date (of the non-random trader's trade) can be traded by the random trader. Visibly, the random traders are not profitable over time whereas the non-random traders are.
	
	The table shows that making use of the predicted class probabilities from the CatBoost classifier improves the trading results. If only trades are placed when \lstinline{lg_inc} is classified with a probability of $>0.4$, then the percentage of trades with profit increases from 55\% to 62\%, the total number of trades reduces to 27\% while the total profit only reduces to 85\% (compared to the trader that always places a trade whenever \lstinline{lg_inc} is classified).
	

	\begin{table}[h!]
		\centering
		\caption{Backtesting results}
		\label{table:results_trading}
		
		\begin{tabular}{lrrrrrrr}
			\toprule
			Type &    No. trades &  \% of trades with profit &     \% of trades with loss &     Total profit \\
			\midrule
			Trade all lg\_inc  &  1628 &  55\% &   45\% & \$8797 \\
			Trade lg\_inc with prob. $>$ 0.40  & 443 & 62\% &  38\% &  \$7450 \\
			\bottomrule
		\end{tabular}
		
	\end{table}%

	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{img/trade_lg_inc_all.png}
		\caption{Cumulative profit over time in QTR1 2019 (all \lstinline{lg_inc} vs. random trades)}
		\label{fig:trade_lg_inc_all}
	\end{figure}


	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{img/trade_lg_inc_40.png}
		\caption{Cumulative profit over time in QTR1 2019 (\lstinline{lg_inc} with $>0.4$ probability vs. random trades)}
		\label{fig:trade_lg_inc_0.4}
	\end{figure}

	\newpage
	\subsubsection{Important Features}
	
	A word cloud with features weighted by feature importance is shown in figure ~\ref{fig:feat_imp}. The feature importance is a normalized CatBoost metric that reflects how much the prediction values change when the feature changes \cite{noauthor_feature_nodate}. In the word cloud, larger words have a larger feature importance.
	
	\begin{figure}[h!]
		\includegraphics[width=\linewidth]{img/feat_imp.png}
		\caption{Word cloud of features weighted by feature importance}
		\label{fig:feat_imp}
	\end{figure}
	
	
	\section{Discussion}
	
	\subsection{Model Performance}
	
	Considering that Lee et al. could achieve an increase in accuracy by 10\% when including text data from 8-K filings in their classification model, the achieved results in this project are good \cite{lee_importance_2014}. The best model of this project also improved the test accuracy by 10\% (compared to the majority classifier). 
	
	Moreover, one can see that the many features used by the model, i.e. the features with a certain feature importance, seem reasonable. The word cloud in figure ~\ref{fig:feat_imp} shows many words that seem relevant for stock price changes, e.g. increase, decrease, disappoint, profit, and improvement. However, there are also words that seem less relevant, e.g. months like April, abbreviations like inc, and other terms like conference. This indicates that the model did well in identifying relevant tokens as features but that it might be mistaken about other tokens.
	
	\subsection{Practical Implications}
	
	Overall, the results seem to indicate that 8-K filings can be useful in generating trading signals. As the backtesting shows, simply placing trades when the model classifies \lstinline{lg_inc} may be sufficient to generate profits in the long-term. However, the backtesting was conducted with simplified assumptions, e.g. the stop loss of 5\%. In practice, we may not always be able to sell the stocks with a stop loss of 5\%, e.g. during pre- or after-market hours. Future projects should investigate whether these assumptions hold in practice. In addition, one might compare the 8-K filing strategy with other trading strategies instead of a random trader. 
	
	\subsection{Future Projects}
		
	Above all, future projects should investigate possibilities to further improve the accuracy and class-wise precision. Three potential projects could be the following. First, one could add other data sources such as stock price time series and fundamental data about the companies. Second, one could compare CatBoost with more complex models such as neural networks with word embeddings like BERT. Third, one could investigate the 8-K filings that are associated with certain percentage changes with methods such as topic modeling to understand better which 8-K filings may actually lead to certain percentage changes such as a \lstinline{lg_inc}.
	

	\section{Conclusion}

	In conclusion, forecasting stock price changes based on 8-K filings has potential for usage in a  trading application. A backtesting with simplified assumptions turned out to be profitable. The CatBoost classifier was shown to be suitable model for this problem. It identified relevant text features from the 8-K filings and outperformed all other baseline models with an accuracy of 30\% on the test data (compared to 20\% with a majority classifier). Its largest class-wise precision was achieved for the class \lstinline{lg_inc}, namely a precision of of 36\%. 
	
\clearpage
\printbibliography
\end{document}